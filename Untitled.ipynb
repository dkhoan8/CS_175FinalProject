{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data directories\n",
      "Attempting to download train_32x32.mat\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified data/svhn/cropped\\train_32x32.mat\n",
      "Saving to train_svhn_imgs.npy file done.\n",
      "Saving to train_svhn_labels.npy file done.\n",
      "Saving to valid_svhn_imgs.npy file done.\n",
      "Saving to valid_svhn_labels.npy file done.\n",
      "Creating data directories\n",
      "Attempting to download test_32x32.mat\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified data/svhn/cropped\\test_32x32.mat\n",
      "Saving to test_svhn_imgs.npy file done.\n",
      "Saving to test_svhn_labels.npy file done.\n",
      "Cropped Files Done!!!\n",
      "Creating data directories\n",
      "Attempting to download train.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified data/svhn/full\\train.tar.gz\n",
      "extract data/svhn/full\\train.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nguye\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\_hl\\dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extract\n",
      "Skipping {}, only images with less than {} numbers are allowed!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-acdd3eda3696>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[0mgenerate_cropped_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m     \u001b[0mgenerate_full_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-acdd3eda3696>\u001b[0m in \u001b[0;36mgenerate_full_files\u001b[1;34m()\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_full_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_svhn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'full'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_validation_spit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-acdd3eda3696>\u001b[0m in \u001b[0;36mcreate_svhn\u001b[1;34m(dataset, master_set)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mnew_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_data_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tar.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_tar_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;34m''' Return the data file '''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-acdd3eda3696>\u001b[0m in \u001b[0;36mhandle_tar_file\u001b[1;34m(file_pointer)\u001b[0m\n\u001b[0;32m    141\u001b[0m                                            OUT_HEIGHT, OUT_WIDTH)\n\u001b[0;32m    142\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Skipping {}, only images with less than {} numbers are allowed!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LABELS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import tarfile\n",
    "from PIL import Image as Image\n",
    "\n",
    "from digit_struct import DigitStruct\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "DATA_PATH = \"data/svhn/\"\n",
    "CROPPED_DATA_PATH = DATA_PATH+\"cropped\"\n",
    "FULL_DATA_PATH = DATA_PATH+\"full\"\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "\n",
    "OUT_HEIGHT = 64\n",
    "OUT_WIDTH = 64\n",
    "NUM_CHANNELS = 3\n",
    "MAX_LABELS = 5\n",
    "\n",
    "last_percent_reported = None\n",
    "\n",
    "\n",
    "def read_data_file(file_name):\n",
    "    file = open(file_name, 'rb')\n",
    "    data = process_data_file(file)\n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_digit_struct(data_path):\n",
    "    struct_file = os.path.join(data_path, \"digitStruct.mat\")\n",
    "    dstruct = DigitStruct(struct_file)\n",
    "    structs = dstruct.get_all_imgs_and_digit_structure()\n",
    "    return structs\n",
    "\n",
    "\n",
    "def extract_data_file(file_name):\n",
    "    tar = tarfile.open(file_name, \"r:gz\")\n",
    "    tar.extractall(FULL_DATA_PATH)\n",
    "    tar.close()\n",
    "\n",
    "\n",
    "def convert_imgs_to_array(img_array):\n",
    "    rows = img_array.shape[0]\n",
    "    cols = img_array.shape[1]\n",
    "    chans = img_array.shape[2]\n",
    "    num_imgs = img_array.shape[3]\n",
    "    scalar = 1 / PIXEL_DEPTH\n",
    "    # Note: not the most efficent way but can monitor what is happening\n",
    "    new_array = np.empty(shape=(num_imgs, rows, cols, chans), dtype=np.float32)\n",
    "    for x in range(0, num_imgs):\n",
    "        # TODO reuse normalize_img here\n",
    "        chans = img_array[:, :, :, x]\n",
    "        # normalize pixels to 0 and 1. 0 is pure white, 1 is pure channel color\n",
    "        norm_vec = (255-chans)*1.0/255.0\n",
    "        # Mean Subtraction\n",
    "        norm_vec -= np.mean(norm_vec, axis=0)\n",
    "        new_array[x] = norm_vec\n",
    "    return new_array\n",
    "\n",
    "\n",
    "def convert_labels_to_one_hot(labels):\n",
    "    labels = (np.arange(NUM_LABELS) == labels[:, None]).astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def process_data_file(file):\n",
    "    data = loadmat(file)\n",
    "    imgs = data['X']\n",
    "    labels = data['y'].flatten()\n",
    "    labels[labels == 10] = 0  # Fix for weird labeling in dataset\n",
    "    labels_one_hot = convert_labels_to_one_hot(labels)\n",
    "    img_array = convert_imgs_to_array(imgs)\n",
    "    return img_array, labels_one_hot\n",
    "\n",
    "\n",
    "def get_data_file_name(master_set, dataset):\n",
    "    if master_set == \"cropped\":\n",
    "        if dataset == \"train\":\n",
    "            data_file_name = \"train_32x32.mat\"\n",
    "        elif dataset == \"test\":\n",
    "            data_file_name = \"test_32x32.mat\"\n",
    "        elif dataset == \"extra\":\n",
    "            data_file_name = \"extra_32x32.mat\"\n",
    "        else:\n",
    "            raise Exception('dataset must be either train, test or extra')\n",
    "    elif master_set == \"full\":\n",
    "        if dataset == \"train\":\n",
    "            data_file_name = \"train.tar.gz\"\n",
    "        elif dataset == \"test\":\n",
    "            data_file_name = \"test.tar.gz\"\n",
    "        elif dataset == \"extra\":\n",
    "            data_file_name = \"extra.tar.gz\"\n",
    "    else:\n",
    "        raise Exception('Master data set must be full or cropped')\n",
    "    return data_file_name\n",
    "\n",
    "\n",
    "def make_data_dirs(master_set):\n",
    "    if master_set == \"cropped\":\n",
    "        if not os.path.exists(CROPPED_DATA_PATH):\n",
    "            os.makedirs(CROPPED_DATA_PATH)\n",
    "    elif master_set == \"full\":\n",
    "        if not os.path.exists(FULL_DATA_PATH):\n",
    "            os.makedirs(FULL_DATA_PATH)\n",
    "    else:\n",
    "        raise Exception('Master data set must be full or cropped')\n",
    "\n",
    "\n",
    "def handle_tar_file(file_pointer):\n",
    "    ''' Extract and return the data file '''\n",
    "    print (\"extract\", file_pointer)\n",
    "    extract_data_file(file_pointer)\n",
    "    extract_dir = os.path.splitext(os.path.splitext(file_pointer)[0])[0]\n",
    "\n",
    "    structs = read_digit_struct(extract_dir)\n",
    "    data_count = len(structs)\n",
    "\n",
    "    img_data = np.zeros((data_count, OUT_HEIGHT, OUT_WIDTH, NUM_CHANNELS),\n",
    "                        dtype='float32')\n",
    "    labels = np.zeros((data_count, MAX_LABELS+1), dtype='int8')\n",
    "\n",
    "    for i in range(data_count):\n",
    "        lbls = structs[i]['label']\n",
    "        file_name = os.path.join(extract_dir, structs[i]['name'])\n",
    "        top = structs[i]['top']\n",
    "        left = structs[i]['left']\n",
    "        height = structs[i]['height']\n",
    "        width = structs[i]['width']\n",
    "        if(len(lbls) < MAX_LABELS):\n",
    "            labels[i] = create_label_array(lbls)\n",
    "            img_data[i] = create_img_array(file_name, top, left, height, width,\n",
    "                                           OUT_HEIGHT, OUT_WIDTH)\n",
    "        else:\n",
    "            print(\"Skipping {}, only images with less than {} numbers are allowed!\").format(file_name, MAX_LABELS)\n",
    "\n",
    "    return img_data, labels\n",
    "\n",
    "\n",
    "def create_svhn(dataset, master_set):\n",
    "    path = DATA_PATH+master_set\n",
    "    data_file_name = get_data_file_name(master_set, dataset)\n",
    "    data_file_pointer = os.path.join(path, data_file_name)\n",
    "\n",
    "    if (not os.path.exists(data_file_pointer)):\n",
    "        ''' Create the data dir structure '''\n",
    "        print(\"Creating data directories\")\n",
    "        make_data_dirs(master_set)\n",
    "    if os.path.isfile(data_file_pointer):\n",
    "        if(data_file_pointer.endswith(\"tar.gz\")):\n",
    "            return handle_tar_file(data_file_pointer)\n",
    "        else:\n",
    "            ''' Use the existing file '''\n",
    "            extract_data = read_data_file(data_file_pointer)\n",
    "            return extract_data\n",
    "    else:\n",
    "        new_file = download_data_file(path, data_file_name)\n",
    "        if(new_file.endswith(\"tar.gz\")):\n",
    "            return handle_tar_file(new_file)\n",
    "        else:\n",
    "            ''' Return the data file '''\n",
    "            return read_data_file(new_file)\n",
    "\n",
    "\n",
    "def download_progress(count, block_size, total_size):\n",
    "    global last_percent_reported\n",
    "    percent = int(count * block_size * 100 / total_size)\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "        last_percent_reported = percent\n",
    "\n",
    "\n",
    "def download_data_file(path, filename, force=False):\n",
    "    base_url = \"http://ufldl.stanford.edu/housenumbers/\"\n",
    "    print(\"Attempting to download\", filename)\n",
    "    saved_file, _ = urlretrieve(base_url + filename, os.path.join(path, filename), reporthook=download_progress)\n",
    "    print (\"\\nDownload Complete!\")\n",
    "    statinfo = os.stat(saved_file)\n",
    "    if statinfo.st_size == get_expected_bytes(filename):\n",
    "        print(\"Found and verified\", saved_file)\n",
    "    else:\n",
    "        raise Exception(\"Failed to verify \" + filename)\n",
    "    return saved_file\n",
    "\n",
    "\n",
    "def get_expected_bytes(filename):\n",
    "    if filename == \"train_32x32.mat\":\n",
    "        byte_size = 182040794\n",
    "    elif filename == \"test_32x32.mat\":\n",
    "        byte_size = 64275384\n",
    "    elif filename == \"extra_32x32.mat\":\n",
    "        byte_size = 1329278602\n",
    "    elif filename == \"test.tar.gz\":\n",
    "        byte_size = 276555967\n",
    "    elif filename == \"train.tar.gz\":\n",
    "        byte_size = 404141560\n",
    "    elif filename == \"extra.tar.gz\":\n",
    "        byte_size = 1955489752\n",
    "    else:\n",
    "        raise Exception(\"Invalid file name \" + filename)\n",
    "    return byte_size\n",
    "\n",
    "\n",
    "def train_validation_spit(train_dataset, train_labels):\n",
    "    train_dataset, validation_dataset, train_labels, validation_labels = train_test_split(train_dataset, train_labels, test_size=0.1, random_state = 42)\n",
    "    return train_dataset, validation_dataset, train_labels, validation_labels\n",
    "\n",
    "\n",
    "def write_npy_file(data_array, lbl_array, data_set_name, data_path):\n",
    "    np.save(os.path.join(DATA_PATH+data_path, data_path+\"_\"+data_set_name+'_imgs.npy'), data_array)\n",
    "    print('Saving to %s_svhn_imgs.npy file done.' % data_set_name)\n",
    "    np.save(os.path.join(DATA_PATH+data_path, data_path+\"_\"+data_set_name+'_labels.npy'), lbl_array)\n",
    "    print('Saving to %s_svhn_labels.npy file done.' % data_set_name)\n",
    "\n",
    "\n",
    "def load_svhn_data(data_type, data_set_name):\n",
    "    # TODO add error handling here\n",
    "    path = DATA_PATH + data_set_name\n",
    "    imgs = np.load(os.path.join(path, data_set_name+'_'+data_type+'_imgs.npy'))\n",
    "    labels = np.load(os.path.join(path, data_set_name+'_'+data_type+'_labels.npy'))\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "def create_label_array(el):\n",
    "    \"\"\"[count, digit, digit, digit, digit, digit]\"\"\"\n",
    "    num_digits = len(el)  # first element of array holds the count\n",
    "    labels_array = np.ones([MAX_LABELS+1], dtype=int) * 10\n",
    "    labels_array[0] = num_digits\n",
    "    for n in range(num_digits):\n",
    "        if el[n] == 10: el[n] = 0  # reassign 0 as 10 for one-hot encoding\n",
    "        labels_array[n+1] = el[n]\n",
    "    return labels_array\n",
    "\n",
    "\n",
    "def create_img_array(file_name, top, left, height, width, out_height, out_width):\n",
    "    img = Image.open(file_name)\n",
    "\n",
    "    img_top = np.amin(top)\n",
    "    img_left = np.amin(left)\n",
    "    img_height = np.amax(top) + height[np.argmax(top)] - img_top\n",
    "    img_width = np.amax(left) + width[np.argmax(left)] - img_left\n",
    "\n",
    "    box_left = np.floor(img_left - 0.1 * img_width)\n",
    "    box_top = np.floor(img_top - 0.1 * img_height)\n",
    "    box_right = np.amin([np.ceil(box_left + 1.2 * img_width), img.size[0]])\n",
    "    box_bottom = np.amin([np.ceil(img_top + 1.2 * img_height), img.size[1]])\n",
    "\n",
    "    img = img.crop((box_left, box_top, box_right, box_bottom)).resize([out_height, out_width], Image.ANTIALIAS)\n",
    "    pix = np.array(img)\n",
    "\n",
    "    norm_pix = (255-pix)*1.0/255.0\n",
    "    norm_pix -= np.mean(norm_pix, axis=0)\n",
    "    return norm_pix\n",
    "\n",
    "\n",
    "def generate_full_files():\n",
    "    train_data, train_labels = create_svhn('train', 'full')\n",
    "    train_data, valid_data, train_labels, valid_labels = train_validation_spit(train_data, train_labels)\n",
    "\n",
    "    write_npy_file(train_data, train_labels, 'train', 'full')\n",
    "    write_npy_file(valid_data, valid_labels, 'valid', 'full')\n",
    "\n",
    "    test_data, test_labels = create_svhn('test', 'full')\n",
    "    write_npy_file(test_data, test_labels, 'test', 'full')\n",
    "\n",
    "    # extra_data, extra_labels = create_svhn('full', 'extra')\n",
    "    print(\"Full Files Done!!!\")\n",
    "\n",
    "\n",
    "def generate_cropped_files():\n",
    "    train_data, train_labels = create_svhn('train', 'cropped')\n",
    "    train_data, valid_data, train_labels, valid_labels = train_validation_spit(train_data, train_labels)\n",
    "\n",
    "    write_npy_file(train_data, train_labels, 'train', 'cropped')\n",
    "    write_npy_file(valid_data, valid_labels, 'valid', 'cropped')\n",
    "\n",
    "    test_data, test_labels = create_svhn('test', 'cropped')\n",
    "    write_npy_file(test_data, test_labels, 'test', 'cropped')\n",
    "    print(\"Cropped Files Done!!!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate_cropped_files()\n",
    "    generate_full_files()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
